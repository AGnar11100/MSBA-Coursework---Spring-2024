{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf805389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Stats libraries\n",
    "import ppscore as pps\n",
    "import statsmodels.api as sm\n",
    "import pingouin as pg\n",
    "from scipy import stats\n",
    "\n",
    "# Feature processing, hyperparameter tuning, and validation libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67744116",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1000\n",
    "pd.options.mode.copy_on_write = True \n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    dirty_hair = pd.read_csv('Predict Hair Fall.csv')\n",
    "except FileNotFoundError:\n",
    "    print('Predict Hair Fall.csv is not in your present working directory')\n",
    "\n",
    "# Clean data\n",
    "dirty_hair.columns = dirty_hair.columns.str.strip().str.replace(\" \", \"_\")\n",
    "dirty_hair = dirty_hair[(dirty_hair['Medical_Conditions'] != 'No Data') & \n",
    "                        (dirty_hair['Medications_&_Treatments'] != 'No Data') & \n",
    "                        (dirty_hair['Nutritional_Deficiencies'] != 'No Data')]\n",
    "clean_hair = dirty_hair.drop_duplicates(subset='Id', keep='first')\n",
    "\n",
    "print(f\"Number of rows with duplicated Id: {dirty_hair['Id'].duplicated().sum()}\")\n",
    "print(f\"After cleaning: {len(clean_hair)} rows remaining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicates(df):\n",
    "    duplicates = df.duplicated()\n",
    "    print(\"\\nDuplicates exist in the DataFrame.\" if duplicates.any() else \"\\nNo duplicates found in the DataFrame.\")\n",
    "\n",
    "def count_outliers(series):\n",
    "    Q1, Q3 = series.quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)].count()\n",
    "    print(f'There are {outliers} outliers in this feature')\n",
    "\n",
    "def cat_map(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = df[column].astype('category').cat.codes\n",
    "    return df\n",
    "\n",
    "def binary_encoding(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == 2:\n",
    "            df[col] = df[col].astype('category').cat.codes\n",
    "    return df\n",
    "\n",
    "def dummy_encoding(df):\n",
    "    return pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = clean_hair.copy()\n",
    "\n",
    "# Categorical features pie chart\n",
    "categorical_df = eda_df.select_dtypes(include='object').apply(lambda col: col.astype('category'))\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(12, 6))\n",
    "for i, (column, data) in enumerate(categorical_df.items()):\n",
    "    counts = data.value_counts()\n",
    "    axes.flat[i].pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140)\n",
    "    axes.flat[i].set_title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for Medical Conditions\n",
    "plt.figure(figsize=(10, 6))\n",
    "eda_df['Medical_Conditions'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Number of Records for Each Medical Condition')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot and histogram for Age\n",
    "plt.figure()\n",
    "sm.qqplot(eda_df['Age'], line='q')\n",
    "plt.title('Age')\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(clean_hair['Age'], bins=5, kde=True)\n",
    "plt.title('Distribution of Age')\n",
    "plt.show()\n",
    "\n",
    "# Outliers and normality checks\n",
    "pg.normality(eda_df['Age'])\n",
    "pg.homoscedasticity(eda_df, dv='Age', group='Hair_Loss')\n",
    "count_outliers(eda_df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a63d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square tests\n",
    "categorical_features = categorical_df.columns.tolist()\n",
    "for col1, col2 in [(col1, col2) for i, col1 in enumerate(categorical_features) for col2 in categorical_features[i+1:]]:\n",
    "    if 'Hair_Loss' in (col1, col2):\n",
    "        chi2_res = pg.chi2_independence(categorical_df, col1, col2)\n",
    "        print(f\"Chi-square test for {col1} and {col2}\")\n",
    "        display(chi2_res[2].style.background_gradient(cmap='Reds', subset='pval'))\n",
    "\n",
    "# Summary of chi-square tests\n",
    "chisq_df = eda_df[['Genetics', 'Hormonal_Changes', 'Medical_Conditions', 'Medications_&_Treatments', 'Nutritional_Deficiencies', 'Stress', 'Poor_Hair_Care_Habits', 'Environmental_Factors', 'Smoking', 'Weight_Loss', 'Hair_Loss']]\n",
    "chisq_results = [(col1, col2, pg.chi2_independence(chisq_df, col1, col2)[2]['pval'][0]) for col1, col2 in [(col1, col2) for i, col1 in enumerate(chisq_df.columns) for col2 in chisq_df.columns[i+1:]] if 'Hair_Loss' in (col1, col2)]\n",
    "chisq_summary = pd.DataFrame(chisq_results, columns=['Feature 1', 'Feature 2', 'p-value'])\n",
    "display(HTML(tabulate(chisq_summary, headers='keys', tablefmt='html')))\n",
    "\n",
    "# Predictive Power Score (PPS)\n",
    "corr_matrix_pps = pps.matrix(eda_df.drop('Id', axis=1))[['x', 'y', 'ppscore']].pivot(index='y', columns='x', values='ppscore')\n",
    "sns.heatmap(corr_matrix_pps, cmap='coolwarm', annot=True)\n",
    "plt.title(\"Predictive Power Score (PPS)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_importance(df):\n",
    "    df = cat_map(df.copy())\n",
    "    y = df['Hair_Loss']\n",
    "    X = df.drop(['Hair_Loss', 'Id'], axis=1)\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
    "    rf_classifier.fit(X, y)\n",
    "    feature_importance = pd.Series(rf_classifier.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    sns.barplot(x=feature_importance, y=feature_importance.index, palette='viridis')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "\n",
    "tree_importance(clean_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee8214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_importance(df):\n",
    "    df = cat_map(df.copy())\n",
    "    scaler = MinMaxScaler()\n",
    "    df['Age'] = scaler.fit_transform(df[['Age']])\n",
    "    y = df['Hair_Loss']\n",
    "    X = df.drop(['Hair_Loss', 'Id'], axis=1)\n",
    "    est = sm.OLS(y, X).fit()\n",
    "    return HTML(est.summary().tables[1].as_html())\n",
    "\n",
    "linear_importance(clean_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b77c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_preprocessing(df, features, linear=False):\n",
    "    df = df.copy()\n",
    "    if linear:\n",
    "        df['Age'] = MinMaxScaler().fit_transform(df[['Age']])\n",
    "    y = df['Hair_Loss']\n",
    "    X = df[features].copy()\n",
    "    X['Stress'] = X['Stress'].map({'High': 1, 'Moderate': 0, 'Low': -1})\n",
    "    X = binary_encoding(X)\n",
    "    X = dummy_encoding(X)\n",
    "    return train_test_split(X, y, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_features = ['Genetics', 'Hormonal_Changes', 'Medical_Conditions', 'Stress', 'Age', 'Poor_Hair_Care_Habits', 'Weight_Loss']\n",
    "X_train_lin, X_test_lin, y_train_lin, y_test_lin = model_preprocessing(clean_hair, linear_features, linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_features = ['Age', 'Nutritional_Deficiencies', 'Medical_Conditions', 'Medications_&_Treatments', 'Stress', 'Environmental_Factors', 'Hormonal_Changes']\n",
    "X_train_tree, X_test_tree, y_train_tree, y_test_tree = model_preprocessing(clean_hair, tree_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290cc5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_params(X_train, X_test, y_train, y_test):\n",
    "    param_grid = {\n",
    "        'C': np.logspace(-3, 3, 100),\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear'],\n",
    "        'max_iter': np.random.randint(100, 1000, 10)\n",
    "    }\n",
    "    f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "    log_reg = GridSearchCV(LogisticRegression(random_state=SEED), param_grid, cv=5, scoring=f2_scorer, verbose=1, n_jobs=-1)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    best_log_reg = log_reg.best_estimator_\n",
    "    y_pred_train, y_pred_test = best_log_reg.predict(X_train), best_log_reg.predict(X_test)\n",
    "    print(f'Logistic Regression F2 Train: {fbeta_score(y_train, y_pred_train, beta=2):.4f}')\n",
    "    print(f'Logistic Regression F2 Test: {fbeta_score(y_test, y_pred_test, beta=2):.4f}')\n",
    "\n",
    "    svc = GridSearchCV(SVC(random_state=SEED), {\n",
    "        'C': np.logspace(-3, 3, 100),\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }, cv=5, scoring=f2_scorer, verbose=1, n_jobs=-1)\n",
    "    svc.fit(X_train, y_train)\n",
    "    best_svc = svc.best_estimator_\n",
    "    y_pred_train, y_pred_test = best_svc.predict(X_train), best_svc.predict(X_test)\n",
    "    print(f'Support Vector F2 Train: {fbeta_score(y_train, y_pred_train, beta=2):.4f}')\n",
    "    print(f'Support Vector F2 Test: {fbeta_score(y_test, y_pred_test, beta=2):.4f}')\n",
    "    return log_reg.best_params_, svc.best_params_\n",
    "\n",
    "lgr_params, svc_params = linear_model_params(X_train_lin, X_test_lin, y_train_lin, y_test_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_model_params(X_train, X_test, y_train, y_test):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "    xgb = GridSearchCV(XGBClassifier(random_state=SEED), param_grid, cv=5, scoring=f2_scorer, verbose=1, n_jobs=-1)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    best_xgb = xgb.best_estimator_\n",
    "    y_pred_train, y_pred_test = best_xgb.predict(X_train), best_xgb.predict(X_test)\n",
    "    print(f'XGBoost F2 Train: {fbeta_score(y_train, y_pred_train, beta=2):.4f}')\n",
    "    print(f'XGBoost F2 Test: {fbeta_score(y_test, y_pred_test, beta=2):.4f}')\n",
    "\n",
    "    param_grid = {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'depth': [3, 4, 5],\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bylevel': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    catboost = GridSearchCV(CatBoostClassifier(random_state=SEED, silent=True), param_grid, cv=5, scoring=f2_scorer, verbose=1, n_jobs=-1)\n",
    "    catboost.fit(X_train, y_train)\n",
    "    best_catboost = catboost.best_estimator_\n",
    "    y_pred_train, y_pred_test = best_catboost.predict(X_train), best_catboost.predict(X_test)\n",
    "    print(f'CatBoost F2 Train: {fbeta_score(y_train, y_pred_train, beta=2):.4f}')\n",
    "    print(f'CatBoost F2 Test: {fbeta_score(y_test, y_pred_test, beta=2):.4f}')\n",
    "    return xgb.best_params_, catboost.best_params_\n",
    "\n",
    "xgb_params, cat_params = tree_model_params(X_train_tree, X_test_tree, y_train_tree, y_test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(**lgr_params),\n",
    "    'Support Vector Machine': SVC(**svc_params, probability=True)\n",
    "}\n",
    "\n",
    "# Plot ROC curves for each model\n",
    "plt.figure(figsize=(10, 8))\n",
    "f2_scores = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_lin, y_train_lin)\n",
    "    y_pred = clf.predict(X_test_lin)\n",
    "    f2_scores[name] = fbeta_score(y_test_lin, y_pred, beta=2)\n",
    "\n",
    "# Plot bar plot with error bars for F2 scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "names, values = list(f2_scores.keys()), list(f2_scores.values())\n",
    "errors = [np.std(cross_val_score(clf, X_train_lin, y_train_lin, scoring=f2_scorer, cv=5)) for clf in classifiers.values()]\n",
    "mean_scores = [np.mean(cross_val_score(clf, X_train_lin, y_train_lin, scoring=f2_scorer, cv=5)) for clf in classifiers.values()]\n",
    "\n",
    "for i, v in enumerate(mean_scores):\n",
    "    plt.text(i, v + 0.01, str(round(v, 2)), ha='left', va='baseline')\n",
    "\n",
    "plt.bar(names, values, yerr=errors, capsize=10, color='skyblue')\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('F2 Score')\n",
    "plt.title('F2 Score of Different Classifiers')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'XGBoostClassifier': XGBClassifier(**xgb_params),\n",
    "    'CatBoostClassifier': CatBoostClassifier(**cat_params, silent=True)\n",
    "}\n",
    "\n",
    "# Plot ROC curves for each model\n",
    "plt.figure(figsize=(10, 8))\n",
    "f2_scores = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_tree, y_train_tree)\n",
    "    y_pred = clf.predict(X_test_tree)\n",
    "    f2_scores[name] = fbeta_score(y_test_tree, y_pred, beta=2)\n",
    "\n",
    "# Plot bar plot with error bars for F2 scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "names, values = list(f2_scores.keys()), list(f2_scores.values())\n",
    "errors = [np.std(cross_val_score(clf, X_train_tree, y_train_tree, scoring=f2_scorer, cv=5)) for clf in classifiers.values()]\n",
    "mean_scores = [np.mean(cross_val_score(clf, X_train_tree, y_train_tree, scoring=f2_scorer, cv=5)) for clf in classifiers.values()]\n",
    "\n",
    "for i, v in enumerate(mean_scores):\n",
    "    plt.text(i, v + 0.01, str(round(v, 2)), ha='left', va='baseline')\n",
    "    \n",
    "plt.bar(names, values, yerr=errors, capsize=10, color='skyblue')\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('F2 Score')\n",
    "plt.title('F2 Score of Different Classifiers')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
