{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent on Simple Linear Regression\n",
    "\n",
    "The following example was modifed based on online source, author id: aw7633"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "x = [30., 40., 40., 50., 60., 70., 70., 70., 80., 90.] List of advertising investment in $1000\n",
    "\n",
    "y = [184.4, 279.1, 244.0, 314.2, 382.2, 450.2, 423.6, 410.2, 500.4, 505.3] List of corresponding sales in $1000\n",
    "\n",
    "The purpose is to identify relationship (hopefully accurate and reliable) between x and y, so that we can project future sales rather accurately based on the projected advertising investment.\n",
    "\n",
    "Let's take a look at the scatter plot first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [30., 40., 40., 50., 60., 70., 70., 70., 80., 90.]\n",
    "y = [184.4, 279.1, 244.0, 314.2, 382.2, 450.2, 423.6, 410.2, 500.4, 505.3]\n",
    "\n",
    "plt.scatter(x, y) # Plot scatter to observe qualitative relationships between x and y.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between x and y seems to be linear. Or at least good enough.\n",
    "A linear relationship between x and y can be written as a linear function: y = a + bx.\n",
    "Typically, x is called independent variable, input variable, or feature; \n",
    "and y is called dependent variable, or output variable.\n",
    "\n",
    "To follow the convention, we rewrite the regression function as follows:\n",
    "h(x) = t0 + t1* x (t0 regresents $\\theta_0$, and so on). \n",
    "\n",
    "In CS, this function is called a hypothesis. \n",
    "However, there are infinitely many choices of t0 and t1 (infiniately many hypothesis).\n",
    "\n",
    "We draw a scatter plot and visually observed that linear relationship might be a good fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Most likely, you want a linear function or line that fits the existing data \n",
    "(called sample in Statistics and training set in CS) the best. That way, we will\n",
    "feel much more comfortable making projections or predictions.\n",
    "\n",
    "However, we have to measure/quantify/define \"the fit\" and \"the best fit\" in the first place.\n",
    "\n",
    "How would you define the fit? \n",
    "\n",
    "We define fit by using the cost function (\"average\" squared errors). \n",
    "\n",
    "In python, Please define a function that takes the list of x values, the list of y values, and list of t = [t0, t1] as input and that the value of cost function as output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x, y, t): # This only works for the case with one feature or input variable (simply speaking, one x)\n",
    "    m = len(y)\n",
    "    SSE = 0\n",
    "    for i in range(m):\n",
    "        SSE += (t[0] + t[1]*x[i] - y[i])**2\n",
    "    return SSE/(2*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "Now that we have defined the cost function which serves as the criterion for us to look for the best choice of t0 and t1. Our goal is to find the optimal value (minimum) of this cost function by updating t0 and t1 repeatly. \n",
    "\n",
    "Next, we need to actually find the optimal values for t0 and t1.\n",
    "Of course, the classic Statistics has shown us a neat formula for computing optimal t0 and t1 (Least Square method). \n",
    "\n",
    "Here, we introduce a different approach,a computational approach to find optimal t.\n",
    "This approach is called (batch) gradient descent algorithm. It might not be as elegant as the statistical approach. But it is much more practical and power in practice.\n",
    "\n",
    "\n",
    "Here is how the gradient descent algorithm works.\n",
    "\n",
    "Step 1: we pick any starting point for t. Typically, t = [0, 0] \n",
    "particularly if x and y are normalized. More on normalization in the next example.\n",
    "\n",
    "Step 2: calculate the value of the cost function based on the chosen t.\n",
    "\n",
    "Step 3: make a small adjustment to t by moving a samll step towards gradient direction.\n",
    "The length of small step is denoted by alpha, officially called learning rate. This give you a t value at current iteration. \n",
    "\n",
    "Step 4: once you update t to the current iteration, plug into the cost function. Compare the cost function value at current iteration with the cost function value at previous iteration of t. (Idealy the cost function will decrease at each iteration)\n",
    "\n",
    "Step 5: Repeat Step 3 and Step 4 until you find the difference of cost function values is small enough. \n",
    "\n",
    "The formula for calculate gradient descent is provided in Microsoft Word document.\n",
    "\n",
    "Define a functin named gradient(x, y, a, t). This function takes in four parameters:\n",
    "   1. list of x values\n",
    "   2. list of y values\n",
    "   3. learning rate alpha denoted by a\n",
    "   4. and list t, note t=[t0,t1]\n",
    "\n",
    "This function returns the list of updated t with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, a, t): # return the updated values for t=[t0, t1]\n",
    "    m = len(y)\n",
    "    RSE = 0\n",
    "    SUM_ex = 0\n",
    "    for i in range(m):\n",
    "        e = t[0] + t[1]*x[i] - y[i]\n",
    "        RSE += e\n",
    "        SUM_ex += e*x[i]\n",
    "    t[0] = t[0] - a/m*RSE\n",
    "    t[1] = t[1] - a/m*SUM_ex\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "The choice of alpha and starting theta values can be tricky. More often than not, \n",
    "the choice may not lead to convergence.\n",
    "\n",
    "Question: what is convergence? \n",
    "value of cost function at current iteration = value of cost function at last iteration\n",
    "\n",
    "To make gradient descent algorithm likely to converge, we can normalize our data.\n",
    "There are different ways of normalizing data. For example, (x - mean)/SD.\n",
    "Here, we will do the following:\n",
    "\n",
    "(xmax - x)/(xmax - xmin)\n",
    "\n",
    "Next we create a function named normalization. This function takes a list of numbers as input.\n",
    "The output of the function is a list of normalized numbers.\n",
    "\n",
    "After normalization, we can run gradient descent on normalized x and y.\n",
    "\n",
    "### <font color = 'red'>Exercise:</font>\n",
    "Write a function named ``normalization(x)`` to implement above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a list of numbers and return a list of normalized number\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Exercise:</font>\n",
    "\n",
    "If we normalize x and y, the t will be the regression coefficient based on normalized x and y, named normed_t. \n",
    "normed_y=normed_t0+normed_t1 * normed_x. \n",
    "\n",
    "Ultimately, we want to find t based on original x and y. If you caculated normed_t, can you find out the formula to transfer back to t? \n",
    "\n",
    "Please write a function named back2Norm. \n",
    "\n",
    "This function takes five input parameters: xmax, xmin, ymax, ymin, and t.\n",
    "t=[t0, t1] contains the optimal values of t0 and t1 based on normalized x and y.\n",
    "This function will return to an updated list of t, which contains the values of t0\n",
    "and t1 based on the original values of x and y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Wrap it up. \n",
    "\n",
    "Write a main function to implement the steps described in **Part 3**. \n",
    "\n",
    "Set starting values of theta to be t = [0, 0]; learning rate a = 0.2; and \n",
    "the number of iteration of gradient descent to be 100.\n",
    "\n",
    "For each iteration, please print the following in the same line:\n",
    "iteration number, current t, current value of cost function, current value of cost function - last value of cost function\n",
    "\n",
    "Observe whether the algorithm converges or not in the end. If not, adjust your learning rate and # of iterations.\n",
    "If your algorithm converges, print out the values of t0 and t1 based on original values of x and y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 [-0.09504279253340941, 1.0373599688192705] 0.0032398164688974343 -2.5875197041426434e-09\n",
      "The regression coefficient is [36.4568158098715, 5.54830656874645]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalization(x): # take a list of numbers and return a list of normalized number\n",
    "    maxi = max(x)\n",
    "    mini = min(x)\n",
    "    return [(maxi - num)/(maxi - mini) for num in x]\n",
    "\n",
    "def cost(x, y, t): # This only works for the case with one feature or input variable\n",
    "    m = len(y)\n",
    "    SSE = 0\n",
    "    for i in range(m):\n",
    "        SSE += (t[0] + t[1]*x[i] - y[i])**2\n",
    "    return SSE/m\n",
    "\n",
    "\n",
    "def gradient(x, y, a, t): # return the updated values for t=[t0, t1]\n",
    "    m = len(y)\n",
    "    RSE = 0\n",
    "    SUM_ex = 0\n",
    "    for i in range(m):\n",
    "        e = t[0] + t[1]*x[i] - y[i]\n",
    "        RSE += e\n",
    "        SUM_ex += e*x[i]\n",
    "    t[0] = t[0] - a/m*RSE\n",
    "    t[1] = t[1] - a/m*SUM_ex\n",
    "    return t\n",
    "\n",
    "def back2Norm(xmax, xmin, ymax, ymin, t): # Once again, this is for SLR with one input variable\n",
    "    t[0] = ymax - t[0]*(ymax - ymin) - t[1]*(ymax - ymin)*xmax/(xmax - xmin)\n",
    "    t[1] = t[1]*(ymax - ymin)/(xmax - xmin)\n",
    "    return t\n",
    "\n",
    "def main():\n",
    "    x0 = [30., 40., 40., 50., 60., 70., 70., 70., 80., 90.]\n",
    "    y0 = [184.4, 279.1, 244.0, 314.2, 382.2, 450.2, 423.6, 410.2, 500.4, 505.3]\n",
    "\n",
    "    x = normalization(x0)\n",
    "    y = normalization(y0)\n",
    "\n",
    "    t = [0, 0]\n",
    "    a = 1.0\n",
    "    Iter = 100\n",
    "\n",
    "    PreviousCost = cost(x,y,t)\n",
    "    \n",
    "    for i in range(Iter):\n",
    "        if i==Iter-1:\n",
    "            print (i, t, cost(x,y,t), cost(x,y,t)- PreviousCost)\n",
    "        PreviousCost = cost(x,y,t)        \n",
    "        t = gradient(x, y, a, t)\n",
    "        \n",
    "\n",
    "    print ('The regression coefficient is', back2Norm(max(x0), min(x0), max(y0), min(y0), t))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6\n",
    "\n",
    "We will plot a line chart that helps us determine whether our gradient descent algorithm converges or not.\n",
    "\n",
    "To that end, in your main function, add a few lines of code:\n",
    "save your iteration number and the corresponding value of Previous Cost - current cost. \n",
    "Plot a line chart with x-axis being the number of interations and y-axis being the decrease in the value of cost function.\n",
    "\n",
    "Observe whether the algorithm converges or not in the end. If not, adjust your learning rate and # of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 [-0.09504279253340941, 1.0373599688192705] 0.0032398164688974343 -2.5875197041426434e-09\n",
      "[36.4568158098715, 5.54830656874645]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalization(x): # take a list of numbers and return a list of normalized number\n",
    "    maxi = max(x)\n",
    "    mini = min(x)\n",
    "    return [(maxi - num)/(maxi - mini) for num in x]\n",
    "\n",
    "def cost(x, y, t): # This only works for the case with one feature or input variable\n",
    "    m = len(y)\n",
    "    SSE = 0\n",
    "    for i in range(m):\n",
    "        SSE += (t[0] + t[1]*x[i] - y[i])**2\n",
    "    return SSE/m\n",
    "\n",
    "\n",
    "def gradient(x, y, a, t): # return the updated values for t=[t0, t1]\n",
    "    m = len(y)\n",
    "    RSE = 0\n",
    "    SUM_ex = 0\n",
    "    for i in range(m):\n",
    "        e = t[0] + t[1]*x[i] - y[i]\n",
    "        RSE += e\n",
    "        SUM_ex += e*x[i]\n",
    "    t[0] = t[0] - a/m*RSE\n",
    "    t[1] = t[1] - a/m*SUM_ex\n",
    "    return t\n",
    "\n",
    "def back2Norm(xmax, xmin, ymax, ymin, t): # Once again, this is for SLR with one input variable\n",
    "    t[0] = ymax - t[0]*(ymax - ymin) - t[1]*(ymax - ymin)*xmax/(xmax - xmin)\n",
    "    t[1] = t[1]*(ymax - ymin)/(xmax - xmin)\n",
    "    return t\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "def main():\n",
    "    x0 = [30., 40., 40., 50., 60., 70., 70., 70., 80., 90.]\n",
    "    y0 = [184.4, 279.1, 244.0, 314.2, 382.2, 450.2, 423.6, 410.2, 500.4, 505.3]\n",
    "\n",
    "    x = normalization(x0)\n",
    "    y = normalization(y0)\n",
    "\n",
    "    t = [0, 0]\n",
    "    a = 1\n",
    "    Iter = 100\n",
    "\n",
    "    Iteration = []\n",
    "    CostDescent = []\n",
    "    \n",
    "    PreviousCost = cost(x,y,t)\n",
    "    \n",
    "    for i in range(Iter):\n",
    "        if i==Iter-1:\n",
    "            print (i, t, cost(x,y,t), cost(x,y,t)- PreviousCost)\n",
    "        Iteration.append(i)\n",
    "        CostDescent.append(-cost(x,y,t) + PreviousCost)        \n",
    "        PreviousCost = cost(x,y,t)        \n",
    "        t = gradient(x, y, a, t)\n",
    "\n",
    "    \n",
    "    print( back2Norm(max(x0), min(x0), max(y0), min(y0), t))\n",
    "\n",
    "# please add the code to plot the line here: \n",
    "\n",
    "    plt.plot(Iteration[1:], CostDescent[1:],'r')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Descent in Cost\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "      \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
